{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "\n",
    "Gradient Descent is a method wherein we use a [[Cost Function]], take the derivative of the cost function (w.r.t. all the independent parameters) and then adjust our parameters a small amount in the negative derivative direction in order to decrease our cost function, thus adjusting the model's parameters to incur lower costs = better predictions.\n",
    "\n",
    "The following is the function for a straight line\n",
    "$$\n",
    "y = \\upalpha x + \\upbeta\n",
    "$$\n",
    "\n",
    "The [[Mean Squared Error - MSE]] for a linear regression prediction would thus be\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}{\\sum_{i=1}^n((\\upalpha x_i + \\upbeta) - \\hat{y}_i)^2}\n",
    "$$\n",
    "Our MSE is our [[Cost Function]], which would be equal to squaring the difference between $y_{label}$ and our arbitrary line $y_{pred} = \\upalpha x + \\upbeta$ for all our $n$ data points.\n",
    "\n",
    "We want to calculate the derivative w.r.t. $\\upalpha$ & $\\upbeta$:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\upalpha} = \\frac{\\partial f}{\\partial \\upalpha}(\\upalpha x_i + \\upbeta - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\upalpha} = 2x_i(\\upalpha x_i + \\upbeta - \\hat{y}_i)\n",
    "$$\n",
    "Now the derivative w.r.t. $\\upbeta$:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\upbeta} = 2(\\upalpha x_i + \\upbeta - \\hat{y}_i)\n",
    "$$\n",
    "---\n",
    "This means that for every data point $i$ we have, we'll get a certain number (positive or negative). We will sum all of these for the respective params $\\upalpha$ & $\\upbeta$ and that should give us a positive or negative value that will tell us a direction we should adjust the params ($\\upalpha$ and $\\upbeta$ in this case) in order to maximise our cost function. Since we want to minimise the cost function we'll use the negative value and then multiple it by a certain amount (the _learning rate_) which should then give us a new value for our params $\\upalpha$ & $\\upbeta$.\n",
    "\n",
    "For learning rate $\\upmu$\n",
    "$$\n",
    "a := a - \\frac{\\upmu}{n} \\sum_{i=1}^n 2x_i(\\upalpha x_i + \\upbeta - \\hat{y}_i )\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\frac{\\upmu}{n} \\sum_{i=1}^n 2(\\upalpha x_i + \\upbeta - \\hat{y}_i )\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Gradient descent is good at finding a local minimum for the function, but it's not as adept at finding a maximum. Other strategies need to be employed to first identify different \"valleys\" in the function and then settling on e.g. using gradient descent to find the minimum of the most promising valley.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc818d",
   "metadata": {},
   "source": [
    "# 2. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1 initialize a linear function we want to approximate\n",
    "alpha = 4.86\n",
    "beta = -140.6\n",
    "# Set random seed\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Step 2 - generate noisy data along the line\n",
    "# x is 1000 data points from 0 to 100\n",
    "x = np.linspace(0, 100, 1000)\n",
    "delta = rng.uniform(low=-70, high=70, size=1000)\n",
    "y = alpha * x + beta + delta\n",
    "y_noiseless = alpha * x + beta\n",
    "\n",
    "# plt.plot(x, y)\n",
    "# plt.title(\"Noisy linear function data\")\n",
    "# plt.show()\n",
    "\n",
    "# Since we have the answer, we don't need to split data into train/test sets\n",
    "# Instead, we proceed to randomly initialize the weights\n",
    "alpha_pred = ( rng.random() * 10 ) - 5\n",
    "beta_pred = ( rng.random() * 10 ) - 5\n",
    "learn_rate = 0.001\n",
    "epochs = 5000\n",
    "\n",
    "norm_x = (x - x.mean()) / x.std()\n",
    "\n",
    "def train(alpha_pred, beta_pred):\n",
    "  old_alpha = alpha_pred\n",
    "  old_beta = beta_pred\n",
    "  alpha_pred -= ( learn_rate  / y.size ) * np.sum(2 * norm_x  * (old_alpha * x + old_beta - y))\n",
    "  beta_pred -= ( learn_rate / y.size ) * np.sum(2 * (old_alpha * x + old_beta - y))\n",
    "  return alpha_pred, beta_pred\n",
    "\n",
    "for i in range(epochs):\n",
    "  alpha_pred, beta_pred = train(alpha_pred, beta_pred)\n",
    "\n",
    "y_pred = alpha_pred * x + beta_pred\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, color=\"blue\", label=\"data points\", s=10)\n",
    "plt.plot(x, y_pred, color=\"red\", label=\"prediction\")\n",
    "plt.plot(x, y_noiseless, color=\"orange\", label=\"true function\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear regression\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
